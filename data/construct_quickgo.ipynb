{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Annotations are redundant, it means that a protein may be annoteated with a term as well as its coarse grained terms.\n",
    "2. All GO terms in quickgo are not alt ids in go-basic.obo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 存在的问题：\n",
    "1. 蛋白质数量还比较少"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gzip\n",
    "import random\n",
    "import pickle\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "\n",
    "from Bio.PDB import PDBParser\n",
    "from Bio.PDB.Polypeptide import PPBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organisms = [\"arabidopsis\", \"fly\", \"mouse\", \"worm\", \"yeast\", \"human\"]\n",
    "# taxons = [\"3702\", \"7227\", \"10090\", \"6239\", \"559292\", \"9606\"]\n",
    "# organisms = [\"fly\", \"mouse\", \"worm\", \"yeast\", \"human\"]\n",
    "# taxons = [\"7227\", \"10090\", \"6239\", \"559292\", \"9606\"]\n",
    "seed = 99\n",
    "\n",
    "#! whether Use microbiology \n",
    "organisms = [\"human\"]\n",
    "taxons = [\"9606\"]\n",
    "\n",
    "#! params\n",
    "processed_go_dir = \"./processed/gograph/\"\n",
    "quickgo_dir = \"./raw/quickgo\"\n",
    "uniprot_dir = \"./raw/uniprot\"\n",
    "uniref_cluster_fp = f\"./raw/uniref/cluster_member_six_module_orgas_50.tsv.gz\"\n",
    "alphafold_dir = \"./raw/alphafold\"\n",
    "out_dir = \"./processed/quickgo\"\n",
    "\n",
    "# evidence code that are excluded\n",
    "excluded_evidence = [\"ISS\", \"ISO\", \"ISA\", \"ISM\", \"IGC\", \"RCA\", \"IEA\"]\n",
    "\n",
    "min_pro_per_term = 30   # the minimum proteins num of each term\n",
    "min_term_depth = {\"bp\":3 , \"cc\":3 , \"mf\":3}      # the minimum depth of reserved terms\n",
    "\n",
    "max_seq_len = 800\n",
    "min_seq_len = 50\n",
    "non_canonical_aa = re.compile(\"[UZOBJX]\")\n",
    "cluster_max_pro_num = 1     # number of proteins in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Annotation:\n",
    "    def __init__(self, db, dbid, goid, evidence, aspect, taxon, organism):\n",
    "        self.db = db\n",
    "        self.dbid = dbid\n",
    "        self.goid = goid\n",
    "        self.evidence = evidence\n",
    "        self.aspect = aspect\n",
    "        self.taxon = taxon\n",
    "        self.organism = organism\n",
    "\n",
    "    def __str__(self):\n",
    "        string = f\"ID: {self.dbid}, GO: {self.goid}, Evidence: {self.evidence}, Aspect: {self.aspect}, Taxon: {self.taxon}\"\n",
    "        return string\n",
    "\n",
    "\n",
    "class Protein:\n",
    "    def __init__(self, dbid, orga):\n",
    "        self.dbid = dbid\n",
    "        self.annots = []\n",
    "        self.orga = orga\n",
    "        self.sequence = None\n",
    "        self.ca_coords = None\n",
    "    \n",
    "    def add_annot(self, annot):\n",
    "        assert annot.dbid == self.dbid\n",
    "        for _annot in self.annots:\n",
    "            #! Note, there may be same GO terms of different sources or evidence for a protein\n",
    "            if annot.goid == _annot.goid:\n",
    "                return\n",
    "        self.annots.append(annot)\n",
    "    \n",
    "    def set_sequence(self, seq):\n",
    "        self.sequence = seq\n",
    "\n",
    "    def set_ca_coords(self, ca_coords):\n",
    "        assert len(self.sequence) == ca_coords.shape[0]\n",
    "        self.ca_coords = ca_coords\n",
    "\n",
    "    def __str__(self):\n",
    "        string = f\"{self.dbid}, {' '.join([a.goid for a in self.annots])}, {self.sequence}\"\n",
    "        return string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Raw GAF Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_zipped_gaf(orga):\n",
    "    fp = os.path.join(quickgo_dir, f\"goa_{orga}.gaf.gz\")\n",
    "    with gzip.open(fp, \"rb\") as f:\n",
    "        lines = [l.decode() for l in f.readlines()]\n",
    "        lines = [l for l in lines if not l.startswith(\"!\")]\n",
    "    \n",
    "    orga_annotations = []\n",
    "    for line in lines:\n",
    "        items = line.split(\"\\t\")\n",
    "        assert items[11] == 'protein'\n",
    "        annot = Annotation(db=items[0], dbid=items[1], goid=items[4], \\\n",
    "            evidence=items[6], aspect=items[8], taxon=items[12], organism=orga)\n",
    "        orga_annotations.append(annot)\n",
    "    return orga_annotations\n",
    "\n",
    "annotations = {}\n",
    "for orga in organisms:\n",
    "    annotations[orga] = read_zipped_gaf(orga)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Annotation of Other Taxons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635148 of 635716 (99.91%) annotations in human are reserved with 9606\n"
     ]
    }
   ],
   "source": [
    "for orga, taxon in zip(organisms, taxons):\n",
    "    valid_annots = [a for a in annotations[orga] if a.taxon == \"taxon:\" + taxon]\n",
    "\n",
    "    pre_annots_num = len(annotations[orga])\n",
    "    cur_annots_num = len(valid_annots)\n",
    "\n",
    "    annotations[orga] = valid_annots\n",
    "    print(f\"{cur_annots_num} of {pre_annots_num} ({round(cur_annots_num/pre_annots_num*100, 2)}%) annotations in {orga} are reserved with {taxon}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Remove Annotation with excluded evidence codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "528969 of 635148 (83.28%) annotations in human are reserved without IEA\n"
     ]
    }
   ],
   "source": [
    "for orga in organisms:\n",
    "    valid_annots = [a for a in annotations[orga] if a.evidence not in excluded_evidence]\n",
    "\n",
    "    pre_annots_num = len(annotations[orga])\n",
    "    cur_annots_num = len(valid_annots)\n",
    "\n",
    "    annotations[orga] = valid_annots\n",
    "    print(f\"{cur_annots_num} of {pre_annots_num} ({round(cur_annots_num/pre_annots_num*100, 2)}%) annotations in {orga} are reserved without IEA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather Annotations into Proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In human, 18850 proteins, 199682 annotaions (10.59 annots per protein).\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "In Annotations, a protein may have several same GO terms of\n",
    "different source or evidence.\n",
    "\"\"\"\n",
    "proteins = defaultdict(dict)\n",
    "for orga in organisms:\n",
    "    for annot in annotations[orga]:\n",
    "        proid = annot.dbid\n",
    "        if proid not in proteins[orga]:\n",
    "            proteins[orga][proid] = Protein(proid, orga)\n",
    "        proteins[orga][proid].add_annot(annot)\n",
    "\n",
    "    annots_num = sum([len(pro.annots) for pro in proteins[orga].values()])\n",
    "    protein_num = len(proteins[orga])\n",
    "    print(f\"In {orga}, {protein_num} proteins, {annots_num} annotaions ({round(annots_num/protein_num, 2)} annots per protein).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect Protein Sequences from Uniprot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In human has 18850 of 18850 (100.0%) has protein sequences in UniProt.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"read fasta files\"\"\"\n",
    "for orga, taxon in zip(organisms, taxons):    \n",
    "    with gzip.open(os.path.join(uniprot_dir, f\"uniprot-{taxon}-{orga}.fasta.gz\"), \"rb\") as f:\n",
    "        lines = [l.decode() for l in f.readlines()]\n",
    "\n",
    "    cur_id = None\n",
    "    for l in lines:\n",
    "        if l[0] == \">\":\n",
    "            _id = l.rstrip().split(\"|\")[1]\n",
    "            if cur_id is not None:\n",
    "                assert len(line_buffer) != 0\n",
    "                seq = re.sub(\"\\s\", \"\", \"\".join(line_buffer))\n",
    "                if cur_id in proteins[orga]:\n",
    "                    proteins[orga][cur_id].set_sequence(seq)\n",
    "\n",
    "            cur_id = _id\n",
    "            line_buffer = []\n",
    "        else:\n",
    "            line_buffer.append(l)\n",
    "    if len(line_buffer) != 0:\n",
    "        seq = re.sub(\"\\s\", \"\", \"\".join(line_buffer))\n",
    "        if cur_id in proteins[orga]:\n",
    "            proteins[orga][cur_id].set_sequence(seq)\n",
    "        line_buffer = []\n",
    "\n",
    "    pre_pro_num = len(proteins[orga])\n",
    "    valid_pros = {dbid: pro for dbid, pro in proteins[orga].items() if pro.sequence is not None}\n",
    "    proteins[orga] = valid_pros\n",
    "    cur_pro_num = len(valid_pros)\n",
    "\n",
    "    print(f\"In {orga} has {cur_pro_num} of {pre_pro_num} ({round(cur_pro_num/pre_pro_num*100, 2)}%) has protein sequences in UniProt.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Remove Proteins of Too Long or Too Short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In human, 15260 of 18850 (81%) proteins reserved, 154760 of 199682 (81%) annotaions reserved (10.14 annots per protein).\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Filter sequences\"\"\"\n",
    "def is_length_valid(pro):\n",
    "    l = len(pro.sequence)\n",
    "    return min_seq_len <= l and l <= max_seq_len\n",
    "\n",
    "for orga in organisms:\n",
    "    pros = {dbid: p for dbid, p in proteins[orga].items() if is_length_valid(p)}\n",
    "\n",
    "    pre_protein_num = len(proteins[orga])\n",
    "    cur_protein_num = len(pros)\n",
    "    pre_annots_num = sum([len(p.annots) for p in proteins[orga].values()])\n",
    "    cur_annots_num = sum([len(p.annots) for p in pros.values()])\n",
    "    print(f\"In {orga}, {cur_protein_num} of {pre_protein_num} ({round(cur_protein_num / pre_protein_num * 100)}%) proteins reserved,\",\n",
    "        f\"{cur_annots_num} of {pre_annots_num} ({round(cur_protein_num / pre_protein_num * 100)}%) annotaions reserved\",\n",
    "        f\"({round(cur_annots_num/cur_protein_num, 2)} annots per protein).\")\n",
    "    \n",
    "    proteins[orga] = pros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Remove Proteins with Non-canonical Amino Acids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In human, 15233 of 15260 (100%) proteins reserved, 154552 of 154760 (100%) annotaions reserved (10.15 annots per protein).\n"
     ]
    }
   ],
   "source": [
    "def is_sequence_valid(pro):\n",
    "    if re.search(non_canonical_aa, pro.sequence) is not None:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "for orga in organisms:\n",
    "    pros = {dbid: p for dbid, p in proteins[orga].items() if is_sequence_valid(p)}\n",
    "\n",
    "    pre_protein_num = len(proteins[orga])\n",
    "    cur_protein_num = len(pros)\n",
    "    pre_annots_num = sum([len(p.annots) for p in proteins[orga].values()])\n",
    "    cur_annots_num = sum([len(p.annots) for p in pros.values()])\n",
    "    print(f\"In {orga}, {cur_protein_num} of {pre_protein_num} ({round(cur_protein_num / pre_protein_num * 100)}%) proteins reserved,\",\n",
    "        f\"{cur_annots_num} of {pre_annots_num} ({round(cur_protein_num / pre_protein_num * 100)}%) annotaions reserved\",\n",
    "        f\"({round(cur_annots_num/cur_protein_num, 2)} annots per protein).\")\n",
    "    \n",
    "    proteins[orga] = pros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect PDBs from AlphaFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Align alphafold files with organisms\"\"\"\n",
    "orga_afdb_fn = {}\n",
    "for fn in os.listdir(alphafold_dir):\n",
    "    fn_taxon = fn.split(\"_\")[1]\n",
    "    if fn_taxon in taxons:\n",
    "        orga = organisms[taxons.index(fn_taxon)]\n",
    "    else:\n",
    "        continue\n",
    "    orga_afdb_fn[orga] = fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15233/15233 [18:28<00:00, 13.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In human, 15069 proteins reserved, 164 protein(s) has no pdb file in alphafold human dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Read tar file\"\"\"\n",
    "parser = PDBParser()\n",
    "builder = PPBuilder()\n",
    "for orga in organisms:\n",
    "    fp = os.path.join(alphafold_dir, orga_afdb_fn[orga])\n",
    "\n",
    "    name_mem_dict = {}\n",
    "    tar = tarfile.open(fp, \"r\")\n",
    "    for tarmem in tar.getmembers():\n",
    "        tarname = tarmem.name\n",
    "        if \"cif.gz\" in tarname:\n",
    "            continue\n",
    "        dbid = tarname.split(\"-\")[1]\n",
    "        name_mem_dict[dbid] = tarmem\n",
    "\n",
    "    pros = {}\n",
    "    no_pdbs = []\n",
    "    for dbid, pro in tqdm(proteins[orga].items()):\n",
    "        if dbid not in name_mem_dict:\n",
    "            no_pdbs.append(dbid)\n",
    "            continue\n",
    "        handler = gzip.open(tar.extractfile(name_mem_dict[dbid]), \"rt\")\n",
    "        \n",
    "        stru = parser.get_structure(dbid, handler)\n",
    "        assert len(stru) == 1   # only one model\n",
    "        assert len(stru[0]) == 1    # only one chain\n",
    "        pp = builder.build_peptides(stru[0]['A'])[0]\n",
    "        sequence = pp.get_sequence()\n",
    "        ca_coords = np.array([ca.coord for ca in pp.get_ca_list()])\n",
    "\n",
    "        try:\n",
    "            assert sequence == pro.sequence\n",
    "        except AssertionError:\n",
    "            no_pdbs.append(dbid)\n",
    "            continue\n",
    "\n",
    "        pro.set_ca_coords(ca_coords)\n",
    "        pros[dbid] = pro\n",
    "\n",
    "    proteins[orga] = pros\n",
    "    print(f\"In {orga}, {len(pros)} proteins reserved, {len(no_pdbs)} protein(s) has no pdb file in alphafold {orga} dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn Protein Dict into List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Turn proteins into list\"\"\"\n",
    "for orga in proteins:\n",
    "    proteins[orga] = proteins[orga].values()\n",
    "proteins = list(chain(*proteins.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Filter Protein with Uniref50. Only one member is kept in each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Parse UniRef clusters\"\"\"\n",
    "pro_clusters = {}\n",
    "\n",
    "uniref = gzip.open(uniref_cluster_fp, \"rb\")\n",
    "df = pd.read_csv(uniref, delimiter=\"\\t\")\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    entry = row[\"Entry\"]\n",
    "    member_string = row[\"Members\"]\n",
    "\n",
    "    for pro_taxon in member_string.strip().split(\";\"):\n",
    "        accessions, _ = pro_taxon.strip().split(\":\")\n",
    "        for _access in accessions.strip().split(\",\"):\n",
    "            pro_clusters[_access] = entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15069 proteins, 14069 clusters.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Gather proteins into clusters\"\"\"\n",
    "cluster_proteins = defaultdict(list)\n",
    "for pro in proteins:\n",
    "    try:\n",
    "        clusterid = pro_clusters[pro.dbid]\n",
    "    except:\n",
    "        print(f\"{pro.dbid} not exists in Uniref clusters of {orga}!\")\n",
    "        continue\n",
    "    cluster_proteins[clusterid].append(pro)\n",
    "\n",
    "cluster_num = len(cluster_proteins)\n",
    "protein_num = sum([len(cluster) for cluster in cluster_proteins.values()])\n",
    "print(f\"{protein_num} proteins, {cluster_num} clusters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14069 proteins reserved, 145676 annotaions reserved (10.35 annots per protein).\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Sample proteins for each cluster\"\"\"\n",
    "random.seed(seed)\n",
    "proteins = []\n",
    "\n",
    "for clusterid, clusterpros in cluster_proteins.items():\n",
    "    selected_pros = random.sample(clusterpros, k=cluster_max_pro_num)\n",
    "    proteins.extend(selected_pros)\n",
    "\n",
    "cur_protein_num = len(proteins)\n",
    "cur_annots_num = sum([len(p.annots) for p in proteins])\n",
    "\n",
    "print(f\"{cur_protein_num} proteins reserved,\",\n",
    "      f\"{cur_annots_num} annotaions reserved\", f\"({round(cur_annots_num/cur_protein_num, 2)} annots per protein).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump Proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Dump proteins\"\"\"\n",
    "#! Comment this block to avoid write wrong proteins \n",
    "# if not os.path.exists(out_dir):\n",
    "#     os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# with open(os.path.join(out_dir, \"proteins.pkl\"), \"wb\") as f:\n",
    "#     pickle.dump(proteins, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load pickle file\"\"\"\n",
    "random.seed(seed)\n",
    "with open(os.path.join(out_dir, \"proteins.pkl\"), \"rb\") as f:\n",
    "    proteins = pickle.load(f)\n",
    "random.shuffle(proteins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Truncate Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Gather proteins into terms\"\"\"\n",
    "term2pros = defaultdict(set)\n",
    "for pro in proteins:\n",
    "    for annot in pro.annots:\n",
    "        goid = annot.goid\n",
    "        term2pros[goid].add(pro.dbid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "648 of 13319 (4.87%) terms are reserved as their member proteins are more than 30\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Filter out terms of too few proteins\"\"\"\n",
    "reserved_terms = set()\n",
    "for term, pros in term2pros.items():\n",
    "    if len(pros) >= min_pro_per_term:\n",
    "        reserved_terms.add(term)\n",
    "\n",
    "print(f\"{len(reserved_terms)} of {len(term2pros)} ({round(len(reserved_terms)/len(term2pros)*100, 2)}%)\",\n",
    "      f\"terms are reserved as their member proteins are more than {min_pro_per_term}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "588 of 648 (90.74%) terms are reserved\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Filter out terms of too shallow\"\"\"\n",
    "def read_term_depth(_dir, dep):\n",
    "    # f = open(os.path.join(_dir, \"term_leaf.tsv\"))\n",
    "    # terms = set([l.strip().split()[0] for l in f.readlines()[1:]])\n",
    "\n",
    "    terms = set()\n",
    "    f = open(os.path.join(_dir, \"term_depth.tsv\"))\n",
    "    for l in f.readlines()[1:]:\n",
    "        t, i, d = l.strip().split()\n",
    "        if int(d) >= dep:\n",
    "            terms.add(t)\n",
    "    \n",
    "    # f = open(os.path.join(_dir, \"term_leaf.tsv\"))\n",
    "    # leaves = set([l.strip().split()[0] for l in f.readlines()[1:]])\n",
    "    # terms = terms & leaves\n",
    "\n",
    "    return terms\n",
    "\n",
    "# bp_valid_terms = read_term(os.path.join(processed_go_dir, 'bp'))\n",
    "# mf_valid_terms = read_term(os.path.join(processed_go_dir, 'mf'))\n",
    "# cc_valid_terms = read_term(os.path.join(processed_go_dir, 'cc'))\n",
    "bp_valid_terms = read_term_depth(os.path.join(processed_go_dir, 'bp'), min_term_depth['bp'])\n",
    "mf_valid_terms = read_term_depth(os.path.join(processed_go_dir, 'mf'), min_term_depth['mf'])\n",
    "cc_valid_terms = read_term_depth(os.path.join(processed_go_dir, 'cc'), min_term_depth['cc'])\n",
    "valid_terms = bp_valid_terms | mf_valid_terms | cc_valid_terms\n",
    "\n",
    "pre_terms_num = len(reserved_terms)\n",
    "reserved_terms = reserved_terms & valid_terms\n",
    "cur_terms_num = len(reserved_terms)\n",
    "\n",
    "print(f\"{cur_terms_num} of {pre_terms_num} ({round(cur_terms_num/pre_terms_num*100, 2)}%)\",\n",
    "      f\"terms are reserved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422 of 588 (71.77%) terms are reserved\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Filter out terms that are ancestors of other terms\"\"\"\n",
    "def read_term_ancestor(_dir, reserved_terms):\n",
    "    term_ances = {}\n",
    "    delete_terms = set()\n",
    "\n",
    "    idx2term = {}\n",
    "    with open(os.path.join(_dir, \"term_index.tsv\"), \"r\") as f:\n",
    "        for l in f.readlines()[1:]:\n",
    "            t, i = l.split(\"\\t\")\n",
    "            idx2term[int(i)] = t\n",
    "\n",
    "    with open(os.path.join(_dir, \"term_ances.tsv\"), \"r\") as f:\n",
    "        for l in f.readlines()[1:]:\n",
    "            t, _, ances = l.split(\"\\t\")\n",
    "            if t in reserved_terms:\n",
    "                for _t in [idx2term[int(i)] for i in ances.strip().split()]:\n",
    "                    if _t in reserved_terms:\n",
    "                        delete_terms.add(_t)\n",
    "    return delete_terms\n",
    "\n",
    "bp_del_terms = read_term_ancestor(os.path.join(processed_go_dir, 'bp'), reserved_terms)\n",
    "mf_del_terms = read_term_ancestor(os.path.join(processed_go_dir, 'mf'), reserved_terms)\n",
    "cc_del_terms = read_term_ancestor(os.path.join(processed_go_dir, 'cc'), reserved_terms)\n",
    "del_terms = bp_del_terms | mf_del_terms | cc_del_terms\n",
    "\n",
    "pre_terms_num = len(reserved_terms)\n",
    "reserved_terms = reserved_terms - del_terms\n",
    "cur_terms_num = len(reserved_terms)\n",
    "\n",
    "print(f\"{cur_terms_num} of {pre_terms_num} ({round(cur_terms_num/pre_terms_num*100, 2)}%)\",\n",
    "      f\"terms are reserved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11781 of 14069 proteins reserved, 40356 of 145676 annotaions reserved (3.43 annots per protein).\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Remove terms\"\"\"\n",
    "pre_protein_num = len(proteins)\n",
    "pre_annots_num = sum([len(pro.annots) for pro in proteins])\n",
    "\n",
    "for pro in proteins:\n",
    "    pro.annots = [_annot for _annot in pro.annots if _annot.goid in reserved_terms]\n",
    "# remove proteins without annotations\n",
    "proteins = [pro for pro in proteins if len(pro.annots) > 0]\n",
    "\n",
    "cur_protein_num = len(proteins)\n",
    "cur_annots_num = sum([len(pro.annots) for pro in proteins])\n",
    "\n",
    "print(f\"{cur_protein_num} of {pre_protein_num} proteins reserved,\",\n",
    "    f\"{cur_annots_num} of {pre_annots_num} annotaions reserved\", f\"({round(cur_annots_num/cur_protein_num, 2)} annots per protein).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store Protein Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(fp, lines, header=None):\n",
    "    with open(fp, \"w\") as f:\n",
    "        if header is not None:\n",
    "            f.write(header)\n",
    "        f.writelines(lines)\n",
    "\n",
    "def serialize(protein):\n",
    "    seq = protein.sequence\n",
    "    ca_coords = protein.ca_coords\n",
    "    coords = \" \".join(list(map(str, ca_coords.reshape(-1).tolist())))\n",
    "    return seq, coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Store seqs and coords\"\"\"\n",
    "pro_seqs = []\n",
    "pro_coords = []\n",
    "\n",
    "for pro in proteins:\n",
    "    dbid = pro.dbid\n",
    "    seq, coords = serialize(pro)\n",
    "\n",
    "    pro_seqs.append(f\"{dbid}\\t{seq}\\n\")\n",
    "    pro_coords.append(f\"{dbid}\\t{coords}\\n\")\n",
    "    \n",
    "write_file(os.path.join(out_dir, \"seqs.tsv\"), pro_seqs)\n",
    "write_file(os.path.join(out_dir, \"coords.tsv\"), pro_coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split BP MF CC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_terms(processed_cat_dir, tgt_dir):\n",
    "    os.makedirs(tgt_dir, exist_ok=True)\n",
    "\n",
    "    \"\"\"Read go terms\"\"\"\n",
    "    term2idx = {}\n",
    "    idx2term = {}\n",
    "    with open(os.path.join(processed_cat_dir, \"term_index.tsv\"), \"r\") as f:\n",
    "        for l in f.readlines()[1:]:\n",
    "            t, i = l.split(\"\\t\")\n",
    "            term2idx[t] = int(i)\n",
    "            idx2term[int(i)] = t\n",
    "\n",
    "    pro_terms = defaultdict(set)\n",
    "    term_pros = defaultdict(set)\n",
    "\n",
    "    for pro in proteins:\n",
    "        terms = [a.goid for a in pro.annots if a.goid in term2idx]\n",
    "        if len(terms) == 0:\n",
    "            continue\n",
    "\n",
    "        pro_terms[pro.dbid] = set(terms)\n",
    "        for t in terms:\n",
    "            term_pros[t].add(pro.dbid)\n",
    "        \n",
    "    return pro_terms, term_pros\n",
    "\n",
    "bp_pro_terms, bp_term_pros = split_terms(os.path.join(processed_go_dir, \"bp\"), os.path.join(out_dir, \"bp\"))\n",
    "mf_pro_terms, mf_term_pros = split_terms(os.path.join(processed_go_dir, \"mf\"), os.path.join(out_dir, \"mf\"))\n",
    "cc_pro_terms, cc_term_pros = split_terms(os.path.join(processed_go_dir, \"cc\"), os.path.join(out_dir, \"cc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6185 214\n",
      "5442 96\n",
      "9954 112\n"
     ]
    }
   ],
   "source": [
    "print(len(bp_pro_terms), len(bp_term_pros))\n",
    "print(len(mf_pro_terms), len(mf_term_pros))\n",
    "print(len(cc_pro_terms), len(cc_term_pros))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8875 2974\n",
      "1251\n",
      "534\n",
      "766\n",
      "423\n",
      "pro:  5122 2369 1306\n",
      "term:  193 194 173\n",
      "\n",
      "6219 2070\n",
      "647\n",
      "349\n",
      "714\n",
      "360\n",
      "pro:  4440 1836 834\n",
      "term:  87 87 78\n",
      "\n",
      "13574 6644\n",
      "1833\n",
      "2025\n",
      "752\n",
      "2034\n",
      "pro:  7431 5595 3072\n",
      "term:  101 101 90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random.seed(seed)\n",
    "\n",
    "def split_data(pro_terms, term_pros, processed_cat_dir):\n",
    "    terms = list(term_pros.keys())\n",
    "    proteins = list(pro_terms.keys())\n",
    "\n",
    "    random.shuffle(terms)\n",
    "    random.shuffle(proteins)\n",
    "\n",
    "    # unseen_proteins = set(random.sample(proteins, k=int(len(proteins) * 0.1)))\n",
    "    unseen_terms = set(random.sample(terms, k=int(len(terms) * 0.1)))\n",
    "    # seen_proteins = set(random.sample(proteins, k=int(len(proteins) * 0.1))) - unseen_proteins\n",
    "    seen_terms = set(random.sample(terms, k=int(len(terms) * 0.1))) - unseen_terms\n",
    "\n",
    "    samples = [(p, t) for p in pro_terms for t in pro_terms[p]]\n",
    "\n",
    "    train_samples = []\n",
    "    test_samples =[]\n",
    "\n",
    "    # 0:  seen proteins, seen terms\n",
    "    # 1:  seen proteins, unseen terms\n",
    "    # 2:  unseen proteins, seen terms\n",
    "    # 3:  unseen proteins, unseen terms        \n",
    "     \n",
    "    for p, t in samples:\n",
    "        if t in seen_terms:\n",
    "            train_samples.append((p, t))\n",
    "        elif t in unseen_terms:\n",
    "            test_samples.append((p, t))    \n",
    "        else:\n",
    "            if random.uniform(0, 1) < 0.8:\n",
    "                train_samples.append((p, t))\n",
    "            else:\n",
    "                test_samples.append((p, t))\n",
    "\n",
    "        # if p in seen_proteins or t in seen_terms:\n",
    "        #     train_samples.append((p, t))\n",
    "        # elif p in unseen_proteins or t in unseen_terms:\n",
    "        #     test_samples.append((p, t))    \n",
    "        # else:\n",
    "        #     if random.uniform(0, 1) < 0.7:\n",
    "        #         train_samples.append((p, t))\n",
    "        #     else:\n",
    "        #         test_samples.append((p, t))\n",
    "\n",
    "    train_proteins = set([p for p, t in train_samples])\n",
    "    test_proteins = set([p for p, t in test_samples])\n",
    "    train_terms = set([t for p, t in train_samples])\n",
    "    test_terms = set([t for p, t in test_samples])\n",
    "\n",
    "    for i, (p, t) in enumerate(test_samples):\n",
    "        if p in train_proteins and t in train_terms:\n",
    "            test_samples[i] = (p, t, 0)\n",
    "        elif p in train_proteins and t not in train_terms:\n",
    "            test_samples[i] = (p, t, 1)\n",
    "        elif p not in train_proteins and t in train_terms:\n",
    "            test_samples[i] = (p, t, 2)\n",
    "        elif p not in train_proteins and t not in train_terms:\n",
    "            test_samples[i] = (p, t, 3)\n",
    "\n",
    "    print(len(train_samples), len(test_samples))\n",
    "    print(len([i for p, t, i in test_samples if i == 0]))\n",
    "    print(len([i for p, t, i in test_samples if i == 1]))\n",
    "    print(len([i for p, t, i in test_samples if i == 2]))\n",
    "    print(len([i for p, t, i in test_samples if i == 3]))\n",
    "\n",
    "    write_file(os.path.join(processed_cat_dir, \"train_pro.tsv\"), [p+\"\\n\" for p in train_proteins])\n",
    "    write_file(os.path.join(processed_cat_dir, \"train_term.tsv\"), [t+\"\\n\" for t in train_terms])\n",
    "    write_file(os.path.join(processed_cat_dir, \"test_pro.tsv\"), [p+\"\\n\" for p in test_proteins])\n",
    "    write_file(os.path.join(processed_cat_dir, \"test_term.tsv\"), [t+\"\\n\" for t in test_terms])\n",
    "    write_file(os.path.join(processed_cat_dir, \"train.tsv\"), [p+'\\t'+t+\"\\n\" for p, t in train_samples], header=\"Protein\\tTerm\\n\")\n",
    "    write_file(os.path.join(processed_cat_dir, \"test.tsv\"), [p+'\\t'+t+\"\\t\"+str(i)+\"\\n\" for p, t, i in test_samples],header=\"Protein\\tTerm\\tType\\n\")\n",
    "\n",
    "    print(\"pro: \", len(train_proteins), len(test_proteins), len(train_proteins&test_proteins))\n",
    "    print(\"term: \", len(train_terms), len(test_terms), len(train_terms&test_terms))\n",
    "    print()\n",
    "\n",
    "split_data(bp_pro_terms, bp_term_pros, os.path.join(out_dir, \"bp\"))\n",
    "split_data(mf_pro_terms, mf_term_pros, os.path.join(out_dir, \"mf\"))\n",
    "split_data(cc_pro_terms, cc_term_pros, os.path.join(out_dir, \"cc\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('workspace')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d87a92dc5e820933ce1158afb1b057ba0389716ff9ef4f91230a9bbd9be5ff60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
